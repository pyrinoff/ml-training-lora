python finetune.py --base_model 'decapoda-research/llama-7b-hf' \
  --data_path './training-data/training_data.json' \
  --output_dir './output-lora-alpaca' \
  --resume_from_checkpoint='adapters/alpaca-lora-7b' \
  --val_set_size 1 \
  --num_epochs=10 \
  --cutoff_len=512 \
  --group_by_length \
  --lora_target_modules='[q_proj,k_proj,v_proj,o_proj]' \
  --lora_r=16 \
  --micro_batch_size=8